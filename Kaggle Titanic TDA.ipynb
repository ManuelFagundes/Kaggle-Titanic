{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26edf56",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-27T01:36:03.227948Z",
     "iopub.status.busy": "2025-10-27T01:36:03.227500Z",
     "iopub.status.idle": "2025-10-27T01:36:15.469722Z",
     "shell.execute_reply": "2025-10-27T01:36:15.467737Z"
    },
    "papermill": {
     "duration": 12.248766,
     "end_time": "2025-10-27T01:36:15.472280",
     "exception": false,
     "start_time": "2025-10-27T01:36:03.223514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.5/834.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "TDA libraries installed successfully!\n",
      "/kaggle/input/titanic/train.csv\n",
      "/kaggle/input/titanic/test.csv\n",
      "/kaggle/input/titanic/gender_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "# =============================================================================\n",
    "# 1. INSTALLATION & IMPORTS\n",
    "# =============================================================================\n",
    "# Install required TDA and ML libraries\n",
    "!pip install ripser gudhi scikit-learn xgboost plotly -q\n",
    "print(\"TDA libraries installed successfully!\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2fb6f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T01:36:15.480166Z",
     "iopub.status.busy": "2025-10-27T01:36:15.479609Z",
     "iopub.status.idle": "2025-10-27T01:36:15.541682Z",
     "shell.execute_reply": "2025-10-27T01:36:15.540417Z"
    },
    "papermill": {
     "duration": 0.068147,
     "end_time": "2025-10-27T01:36:15.543803",
     "exception": false,
     "start_time": "2025-10-27T01:36:15.475656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c628ea31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T01:36:15.551959Z",
     "iopub.status.busy": "2025-10-27T01:36:15.551571Z",
     "iopub.status.idle": "2025-10-27T01:36:30.010373Z",
     "shell.execute_reply": "2025-10-27T01:36:30.008774Z"
    },
    "papermill": {
     "duration": 14.465324,
     "end_time": "2025-10-27T01:36:30.012440",
     "exception": false,
     "start_time": "2025-10-27T01:36:15.547116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTING COMPLETE ENHANCED TDA PIPELINE\n",
      "FINAL RESULTS SUMMARY\n",
      "Cross-Validation Score: 0.8260\n",
      "Test Predictions - Survived: 161/418 (0.385)\n",
      "Training Survival Rate: 0.384\n",
      "Predicted Survival Rate: 0.385\n",
      "High-confidence predictions: 345/418 (82.5%)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # COMPLETE ENHANCED TDA PIPELINE FOR TITANIC\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, model_selection, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TDA libraries\n",
    "try:\n",
    "    import ripser\n",
    "    RIPSER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    RIPSER_AVAILABLE = False\n",
    "\n",
    "# %%\n",
    "def load_and_preprocess_titanic():\n",
    "    train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "    test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n",
    "    \n",
    "    def preprocess_data(df):\n",
    "        df_clean = df.copy()\n",
    "        df_clean['Age'] = df_clean['Age'].fillna(df_clean['Age'].median())\n",
    "        df_clean['Fare'] = df_clean['Fare'].fillna(df_clean['Fare'].median())\n",
    "        df_clean['Embarked'] = df_clean['Embarked'].fillna('S')\n",
    "        df_clean['FamilySize'] = df_clean['SibSp'] + df_clean['Parch'] + 1\n",
    "        df_clean['IsAlone'] = (df_clean['FamilySize'] == 1).astype(int)\n",
    "        \n",
    "        df_clean['Title'] = df_clean['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Dr': 5, 'Rev': 5, 'Col': 5, 'Major': 5}\n",
    "        df_clean['Title'] = df_clean['Title'].map(title_mapping).fillna(5)\n",
    "        \n",
    "        df_clean['HasCabin'] = (~df_clean['Cabin'].isna()).astype(int)\n",
    "        df_clean['Sex'] = df_clean['Sex'].map({'male': 0, 'female': 1})\n",
    "        df_clean['Embarked'] = df_clean['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "        return df_clean\n",
    "    \n",
    "    train_clean = preprocess_data(train_df)\n",
    "    test_clean = preprocess_data(test_df)\n",
    "    \n",
    "    FEATURE_COLUMNS = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'IsAlone', 'Title', 'HasCabin', 'Embarked']\n",
    "    X_train = train_clean[FEATURE_COLUMNS].values\n",
    "    X_test = test_clean[FEATURE_COLUMNS].values\n",
    "    y_train = train_clean['Survived'].values\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train_normalized = scaler.fit_transform(X_train)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "    \n",
    "    return train_clean, test_clean, X_train, X_test, X_train_normalized, X_test_normalized, y_train\n",
    "\n",
    "train_clean, test_clean, X_train, X_test, X_train_normalized, X_test_normalized, y_train = load_and_preprocess_titanic()\n",
    "\n",
    "# %%\n",
    "class RobustTDAExtractor:\n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def extract_tda_features(self, X_normalized):\n",
    "        if not RIPSER_AVAILABLE:\n",
    "            return self._create_fallback_features(X_normalized)\n",
    "        \n",
    "        tda_features = []\n",
    "        for i in range(len(X_normalized)):\n",
    "            try:\n",
    "                passenger_point = X_normalized[i].reshape(1, -1)\n",
    "                distances = np.linalg.norm(X_normalized - passenger_point, axis=1)\n",
    "                n_neighbors = min(20, len(X_normalized))\n",
    "                neighbor_indices = np.argpartition(distances, n_neighbors)[:n_neighbors]\n",
    "                neighborhood = X_normalized[neighbor_indices]\n",
    "                \n",
    "                diagrams = ripser.ripser(neighborhood, maxdim=1)['dgms']\n",
    "                h0_diagram = diagrams[0]\n",
    "                \n",
    "                if len(h0_diagram) > 0:\n",
    "                    h0_lifetimes = np.clip(h0_diagram[:, 1] - h0_diagram[:, 0], 0, 5.0)\n",
    "                    h0_persistence = np.max(h0_lifetimes) if len(h0_lifetimes) > 0 else 0.0\n",
    "                    h0_components = len(h0_diagram)\n",
    "                    h0_std = np.std(h0_lifetimes) if len(h0_lifetimes) > 0 else 0.0\n",
    "                else:\n",
    "                    h0_persistence, h0_components, h0_std = 0.0, 0, 0.0\n",
    "                \n",
    "                h1_diagram = diagrams[1] if len(diagrams) > 1 else np.array([])\n",
    "                h1_loops = len(h1_diagram) if len(h1_diagram) > 0 else 0\n",
    "                \n",
    "                if len(distances) > 1:\n",
    "                    avg_distance = np.mean(distances[neighbor_indices[1:]])\n",
    "                    local_density = 1.0 / (avg_distance + 0.1)\n",
    "                    local_density = min(local_density, 10.0)\n",
    "                else:\n",
    "                    local_density = 0.0\n",
    "                \n",
    "                tda_features.append([h0_persistence, h0_components, h0_std, h1_loops, local_density])\n",
    "            except Exception:\n",
    "                tda_features.append([0.0, 0, 0.0, 0, 0.0])\n",
    "        \n",
    "        self.feature_names = ['tda_h0_persistence', 'tda_h0_components', 'tda_h0_std', 'tda_h1_loops', 'tda_local_density']\n",
    "        return np.nan_to_num(np.array(tda_features), nan=0.0, posinf=10.0, neginf=0.0)\n",
    "    \n",
    "    def _create_fallback_features(self, X_normalized):\n",
    "        statistical_features = []\n",
    "        for i in range(len(X_normalized)):\n",
    "            passenger_point = X_normalized[i]\n",
    "            distances = np.linalg.norm(X_normalized - passenger_point, axis=1)\n",
    "            n_neighbors = min(15, len(X_normalized))\n",
    "            neighbor_distances = np.partition(distances, n_neighbors)[:n_neighbors]\n",
    "            stats = [\n",
    "                np.mean(neighbor_distances), len(neighbor_distances), np.std(neighbor_distances),\n",
    "                len(neighbor_distances[neighbor_distances < 1.0]),\n",
    "                1.0 / (np.mean(neighbor_distances[1:]) + 0.1) if len(neighbor_distances) > 1 else 0.0\n",
    "            ]\n",
    "            statistical_features.append(stats)\n",
    "        self.feature_names = ['stat_mean_dist', 'stat_neighbor_count', 'stat_std_dist', 'stat_dense_count', 'stat_inv_density']\n",
    "        return np.array(statistical_features)\n",
    "\n",
    "# %%\n",
    "class ConsistentFeatureEngineer:\n",
    "    def engineer_features(self, train_df, test_df):\n",
    "        combined_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "        combined_advanced = self._engineer_advanced_features(combined_df)\n",
    "        train_advanced = combined_advanced.iloc[:len(train_df)].copy()\n",
    "        test_advanced = combined_advanced.iloc[len(train_df):].copy()\n",
    "        \n",
    "        X_train = self._prepare_features(train_advanced)\n",
    "        X_test = self._prepare_features(test_advanced)\n",
    "        X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "        \n",
    "        return X_train.values, X_test.values, list(X_train.columns)\n",
    "    \n",
    "    def _engineer_advanced_features(self, df):\n",
    "        df_advanced = df.copy()\n",
    "        df_advanced['FamilySize'] = df_advanced['SibSp'] + df_advanced['Parch'] + 1\n",
    "        df_advanced['IsAlone'] = (df_advanced['FamilySize'] == 1).astype(int)\n",
    "        df_advanced['Title'] = df_advanced['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4}\n",
    "        df_advanced['Title'] = df_advanced['Title'].map(title_mapping).fillna(5)\n",
    "        df_advanced['AgeGroup'] = pd.cut(df_advanced['Age'], bins=[0, 12, 18, 35, 50, 100], labels=[1, 2, 3, 4, 5]).fillna(3).astype(int)\n",
    "        df_advanced['FarePerPerson'] = df_advanced['Fare'] / df_advanced['FamilySize']\n",
    "        df_advanced['FarePerPerson'] = df_advanced['FarePerPerson'].replace([np.inf, -np.inf], 0)\n",
    "        df_advanced['HasCabin'] = (~df_advanced['Cabin'].isna()).astype(int)\n",
    "        df_advanced['TicketLength'] = df_advanced['Ticket'].apply(len)\n",
    "        return df_advanced\n",
    "    \n",
    "    def _prepare_features(self, df):\n",
    "        feature_columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'IsAlone', 'Title', 'HasCabin', 'AgeGroup', 'FarePerPerson', 'TicketLength']\n",
    "        available_features = [f for f in feature_columns if f in df.columns]\n",
    "        X = df[available_features].copy().fillna(0)\n",
    "        return X\n",
    "\n",
    "# %%\n",
    "class RobustEnsemble:\n",
    "    def __init__(self):\n",
    "        self.scaler = preprocessing.StandardScaler()\n",
    "        self.ensemble = None\n",
    "        self.feature_importance = None\n",
    "        \n",
    "    def train_ensemble(self, X_train, y_train):\n",
    "        X_clean = np.nan_to_num(X_train, nan=0.0, posinf=10.0, neginf=0.0)\n",
    "        X_scaled = self.scaler.fit_transform(X_clean)\n",
    "        \n",
    "        models = {\n",
    "            'xgb': XGBClassifier(n_estimators=150, max_depth=6, learning_rate=0.1, random_state=42, eval_metric='logloss'),\n",
    "            'rf': RandomForestClassifier(n_estimators=150, max_depth=8, random_state=42),\n",
    "            'lgb': LGBMClassifier(n_estimators=150, max_depth=6, learning_rate=0.1, random_state=42, verbose=-1)\n",
    "        }\n",
    "        \n",
    "        best_models = []\n",
    "        for name, model in models.items():\n",
    "            cv_scores = cross_val_score(model, X_scaled, y_train, cv=5, scoring='accuracy')\n",
    "            mean_score = np.mean(cv_scores)\n",
    "            best_models.append((name, model))\n",
    "        \n",
    "        self.ensemble = VotingClassifier(estimators=best_models, voting='soft')\n",
    "        self.ensemble.fit(X_scaled, y_train)\n",
    "        \n",
    "        for name, model in best_models:\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                self.feature_importance = model.feature_importances_\n",
    "                break\n",
    "        \n",
    "        final_scores = cross_val_score(self.ensemble, X_scaled, y_train, cv=5, scoring='accuracy')\n",
    "        return np.mean(final_scores)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_clean = np.nan_to_num(X, nan=0.0, posinf=10.0, neginf=0.0)\n",
    "        X_scaled = self.scaler.transform(X_clean)\n",
    "        predictions = self.ensemble.predict(X_scaled)\n",
    "        probabilities = self.ensemble.predict_proba(X_scaled)[:, 1]\n",
    "        return predictions, probabilities\n",
    "\n",
    "# %%\n",
    "print(\"EXECUTING COMPLETE ENHANCED TDA PIPELINE\")\n",
    "\n",
    "feature_engineer = ConsistentFeatureEngineer()\n",
    "X_train_advanced, X_test_advanced, feature_names = feature_engineer.engineer_features(train_clean, test_clean)\n",
    "\n",
    "tda_extractor = RobustTDAExtractor()\n",
    "X_train_tda = tda_extractor.extract_tda_features(X_train_normalized)\n",
    "X_test_tda = tda_extractor.extract_tda_features(X_test_normalized)\n",
    "\n",
    "X_train_combined = np.hstack([X_train_advanced, X_train_tda])\n",
    "X_test_combined = np.hstack([X_test_advanced, X_test_tda])\n",
    "all_feature_names = feature_names + tda_extractor.feature_names\n",
    "\n",
    "ensemble_model = RobustEnsemble()\n",
    "cv_score = ensemble_model.train_ensemble(X_train_combined, y_train)\n",
    "\n",
    "final_predictions, final_probabilities = ensemble_model.predict(X_test_combined)\n",
    "\n",
    "final_submission = pd.DataFrame({'PassengerId': test_clean['PassengerId'], 'Survived': final_predictions})\n",
    "final_submission.to_csv('enhanced_tda_titanic_submission.csv', index=False)\n",
    "\n",
    "# %%\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(f\"Cross-Validation Score: {cv_score:.4f}\")\n",
    "print(f\"Test Predictions - Survived: {final_predictions.sum()}/{len(final_predictions)} ({final_predictions.mean():.3f})\")\n",
    "print(f\"Training Survival Rate: {y_train.mean():.3f}\")\n",
    "print(f\"Predicted Survival Rate: {final_predictions.mean():.3f}\")\n",
    "\n",
    "high_confidence = np.sum((final_probabilities > 0.7) | (final_probabilities < 0.3))\n",
    "print(f\"High-confidence predictions: {high_confidence}/{len(final_predictions)} ({high_confidence/len(final_predictions):.1%})\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.04145,
   "end_time": "2025-10-27T01:36:31.039426",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-27T01:35:58.997976",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
